# importing all the necessary packages 

import seaborn
import numpy as np
import geopandas as gpd
import pandas as pd
import matplotlib as mpl
import pysal
import mapclassify
import matplotlib.pyplot as plt
import pylab as pl
%matplotlib inline
mpl.rcParams['figure.figsize'] = (15, 10)

# Load profiles and boroughs geojson and merge
profiles = pd.read_csv('lb_profiles.csv')
lb = gpd.read_file('london_boroughs.geojson')
lb_map = lb.merge(profiles)

# Performing a spatial join to aggregate house points to the boroughs. 
# We create columns for mean price, summed price, summed floor area ( to create average price per floor area)
# We count the house points in each borough

hp3 = gpd.sjoin(hp2,lb_map)
hp4 = hp3.groupby('NAME').agg({'price':['mean','median','sum'],'fl_area':'sum','ID':'count'})
hp4.columns = ['Mean_Price','Median_Price','Sum_Price','Sum_Floor_Area','House_Count']
hp4 = hp4.reset_index()
hp4['Ave_Area_Price'] = hp4['Sum_Price']/hp4['Sum_Floor_Area']
hp4

# Now merge the hp4 file to the boroughs dataset for housing density calculations
# Multiply the HECTARES value by 0.01 to obtain final measure in units per km^2
lb2 = lb_map.merge(hp4)
lb2['House_Dens'] = lb2['House_Count']/(lb2['HECTARES']*.01)
lb2

# Adding an equal interval classifier to give equal widths to the House_Dens value for each borough
ei5 = mapclassify.EqualInterval(lb2.House_Dens, k=5)
ei5

# We can now plot the House density of each borough to get a picture of the concentration of houses in London Boroughs
f, ax = plt.subplots(1, figsize=(15, 10)) 
lb2.plot(ax=ax, column='House_Dens', legend=True, cmap='Blues', scheme='EqualInterval', k=5, edgecolor='white')
ax.set_axis_off() #Remove axes from plot 
ax.set_title('House Point Density (units per sq. km.)') #Plot title text
plt.axis('equal') #Set x and y axes to be equal size
plt.show()

----------------------------------------------------------------------------------------------------------------------
# Machine Learning process
----------------------------------------------------------------------------------------------------------------------
# Load housing price data
hp = pd.read_csv('London_housing_price.csv',dtype=float)
hp['price'] = hp['price']
hp2 = gpd.GeoDataFrame(
    hp, geometry=gpd.points_from_xy(x=hp.east, y=hp.north)
)
hp2.crs = lb.crs

#Scaling
from sklearn.preprocessing import StandardScaler
x_scaler = StandardScaler()
x_scaler.fit(hp[['east','north','fl_area']])
X = x_scaler.transform(hp[['east','north','fl_area']]) # z-scored predictors

# We will use the Knearest neighbours algorithm in ML
from sklearn.neighbors import KNeighborsRegressor as NN
reg_object = NN(n_neighbors=6,weights='uniform',p=2) # p=2 for euclidian distance
price = hp['price']/1000.0 
reg_object.fit(X,price)

# optimising tuning parameter
from sklearn.metrics import mean_absolute_error, make_scorer
mae = make_scorer(mean_absolute_error, greater_is_better=False)

from sklearn.model_selection import GridSearchCV
opt_nn = GridSearchCV(
    estimator = NN(),
    scoring = mae,
    param_grid = {
        'n_neighbors':range(1,35),
        'weights':['uniform','distance'],
        'p':[1,2]})

opt_nn.fit(X,price)

# Pipeline
from sklearn.pipeline import Pipeline
pipe = Pipeline([('zscores',StandardScaler()), ('NNreg',NN(n_neighbors=6,weights='uniform',p=2))])
pipe.fit(hp[['east','north','fl_area']],price)

pipe = Pipeline([('zscores',StandardScaler()),('NNreg',NN())])

opt_nn2 = GridSearchCV(
    estimator = pipe,
    scoring = mae,
    param_grid = {
        'NNreg__n_neighbors':range(1,35),
        'NNreg__weights':['uniform','distance'],
        'NNreg__p':[1,2]})

opt_nn2.fit(hp[['east','north','fl_area']],price)


# creating a  mesh for the area of the data
east_mesh, north_mesh = np.meshgrid(np.linspace(505000,555800,100), np.linspace(158400,199900,100))
fl_mesh = np.zeros_like(east_mesh)
fl_mesh[:,:] = np.mean(hp['fl_area'])

# Now to put all three variables into a predict method
grid_predictor_vars = np.array([east_mesh.ravel(), north_mesh.ravel(),fl_mesh.ravel()]).T
hp_pred = opt_nn2.predict(grid_predictor_vars)
hp_mesh = hp_pred.reshape(east_mesh.shape)

# Now we can make a 3D plot of predicted house prices for a house with mean floor are = 92.88
from mpl_toolkits.mplot3d import Axes3D

fig = pl.figure()
ax = Axes3D(fig)
ax.plot_surface(east_mesh, north_mesh, hp_mesh, rstride=1, cstride=1, cmap='YlOrBr',lw=0.01)
ax.set_xlabel('Easting')
ax.set_ylabel('Northing')
ax.set_zlabel('Price at Mean Floor Area')
pl.show()

# ----------------------------

def surf3d(pipe_model,fl_area):
    east_mesh, north_mesh = np.meshgrid(
        np.linspace(505000,555800,100),
        np.linspace(158400,199900,100))
    fl_mesh = np.zeros_like(east_mesh)
    fl_mesh[:,:] = fl_area
    grid_predictor_vars = np.array([east_mesh.ravel(),north_mesh.ravel(),fl_mesh.ravel()]).T
    hp_pred = pipe_model.predict(grid_predictor_vars)
    hp_mesh = hp_pred.reshape(east_mesh.shape)
    fig = pl.figure()
    ax = Axes3D(fig)
    ax.plot_surface(east_mesh, north_mesh, hp_mesh, rstride=1, cstride=1, cmap='YlOrBr',lw=0.01)
    ax.set_xlabel('Easting')
    ax.set_ylabel('Northing')
    ax.set_zlabel('Price at Floor Area')
    #pl.show()
    return
    
# An example of how to use the model for apartments of fl_area=75

surf3d(opt_nn2,75.0)

# Same can be done for any fl_area you would like
